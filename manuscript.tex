% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  man]{apa6}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\ifLuaTeX
\usepackage[bidi=basic]{babel}
\else
\usepackage[bidi=default]{babel}
\fi
\babelprovide[main,import]{english}
% get rid of language-specific shorthands (see #6817):
\let\LanguageShortHands\languageshorthands
\def\languageshorthands#1{}
% Manuscript styling
\usepackage{upgreek}
\captionsetup{font=singlespacing,justification=justified}

% Table formatting
\usepackage{longtable}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\centering\begin{threeparttable}}
%   {\end{threeparttable}\end{landscape}}
\newenvironment{lltable}{\begin{landscape}\centering\begin{ThreePartTable}}{\end{ThreePartTable}\end{landscape}}

% Enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand{\getlongtablewidth}{\begingroup \ifcsname LT@\roman{LT@tables}\endcsname \global\longtablewidth=0pt \renewcommand{\LT@entry}[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}\@nameuse{LT@\roman{LT@tables}} \fi \endgroup}

% \setlength{\parindent}{0.5in}
% \setlength{\parskip}{0pt plus 0pt minus 0pt}

% Overwrite redefinition of paragraph and subparagraph by the default LaTeX template
% See https://github.com/crsh/papaja/issues/292
\makeatletter
\renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-1em}%
  {\normalfont\normalsize\bfseries\itshape\typesectitle}}

\renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
  {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
  {-\z@\relax}%
  {\normalfont\normalsize\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
\makeatother

% \usepackage{etoolbox}
\makeatletter
\patchcmd{\HyOrg@maketitle}
  {\section{\normalfont\normalsize\abstractname}}
  {\section*{\normalfont\normalsize\abstractname}}
  {}{\typeout{Failed to patch abstract.}}
\patchcmd{\HyOrg@maketitle}
  {\section{\protect\normalfont{\@title}}}
  {\section*{\protect\normalfont{\@title}}}
  {}{\typeout{Failed to patch title.}}
\makeatother

\usepackage{xpatch}
\makeatletter
\xapptocmd\appendix
  {\xapptocmd\section
    {\addcontentsline{toc}{section}{\appendixname\ifoneappendix\else~\theappendix\fi\\: #1}}
    {}{\InnerPatchFailed}%
  }
{}{\PatchFailed}
\keywords{body position, motor development, everyday experiences, sitting, machine learning\newline\indent Word count: X}
\DeclareDelayedFloatFlavor{ThreePartTable}{table}
\DeclareDelayedFloatFlavor{lltable}{table}
\DeclareDelayedFloatFlavor*{longtable}{table}
\makeatletter
\renewcommand{\efloat@iwrite}[1]{\immediate\expandafter\protected@write\csname efloat@post#1\endcsname{}}
\makeatother
\usepackage{csquotes}
\raggedbottom
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Full-day, in home validation of infant body position measurements from inertial sensors},
  pdfauthor={John M. Franchak1, Maximilian Tang1, Hailey Rousey1, \& Chuan Luo1},
  pdflang={en-EN},
  pdfkeywords={body position, motor development, everyday experiences, sitting, machine learning},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Full-day, in home validation of infant body position measurements from inertial sensors}
\author{John M. Franchak\textsuperscript{1}, Maximilian Tang\textsuperscript{1}, Hailey Rousey\textsuperscript{1}, \& Chuan Luo\textsuperscript{1}}
\date{}


\shorttitle{Infant position in the home}

\authornote{

Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

Enter author note here.

Correspondence concerning this article should be addressed to John M. Franchak, UC Riverside Department of Psychology, 900 University Avenue, Riverside, CA 92521. E-mail: \href{mailto:franchak@ucr.edu}{\nolinkurl{franchak@ucr.edu}}

}

\affiliation{\phantom{0}}

\abstract{%
Abstract
}



\begin{document}
\maketitle

From moment to moment, infants' movements facilitate and constrain how they can interact with their surroundings. Changes in \emph{body position}---whether infants are supine on their backs, prone on their bellies, sitting, upright, or held by a caregiver---have immediate consequences for vision, object exploration, and social interaction. When sitting and upright, infants have a better view of faces and distant objects compared to their view while in a prone position (Franchak et al., 2018; Kretch et al., 2014; Luo \& Franchak, 2020). Infants struggle to manipulate objects while supine and prone, but sitting affords object exploration (Soska \& Adolph, 2014). Upright walking changes how infants and caregivers interact compared with crawling in a prone position; while walking infants move farther away, share toys in different ways, and hear different language from caregivers (Chen et al., 2022; Karasik et al., 2011, 2014; West \& Iverson, 2021). As infants grow older and acquire new abilities, such as independent sitting and walking, the spend more time sitting and upright and less time held, supine, and prone (Adolph \& Tamis-LeMonda, 2014; Franchak et al., 2018; Franchak, 2019; Thurman \& Corbetta, 2017). Thus, characterizing individual differences in the day-to-day accumulation of body position experiences informs developmental theory by revealing differential opportunities for learning (Franchak, 2020).

In this paper, we present an inertial sensing method to classify infant body position from moment-to-moment across an entire day, and validate its accuracy using over 100 hours of video recorded across 35 in-home data collection sessions. Our method takes inspiration from a more mature technology: Long-form audio recordings of infants' language experiences using wearable audio recorders. We begin by describing the impact of long-form audio recordings on research in developmental psychology, and identify the key features that should be replicated in long-form recordings of motor behavior. Next, we review the current state-of-the-art in measuring infant motor behavior---video and survey data---and their limitations in capturing real-time, full-day behavior. Finally, we discuss the advantages of using inertial sensing to classify motor behavior. Despite promising past results in brief, supervised sessions (Airaksinen et al., 2022, 2020; Franchak et al., 2021), the current investigation takes a needed step forward by testing accuracy over long, unsupervised recordings. Although in the current investigation we focus on classification of body position, this approach can be extended to categorize other aspects of movement, such as locomotion (Airaksinen et al., 2020) and infant-caregiver contact (Yao et al., 2019). Accompanying our paper, we have created a database {[}CITE{]} containing synchronized video and IMU data, allowing researchers to annotate and train models for behaviors beyond body position.

\hypertarget{inspiration-from-long-form-audio-methods}{%
\subsection{Inspiration from Long-Form Audio Methods}\label{inspiration-from-long-form-audio-methods}}

The LENA® recorder is a commercial device that is worn by infants in a custom shirt pocket that has sufficient battery life and storage to record for an entire day. Closed-source LENA® algorithms analyze the audio recordings to provide automatic counts of useful metrics, such as the number of words spoken by adults in the vicinity of the participant. Other long-form audio methods rely on custom-built recorders (Wass et al., 2022), apply alternative classification algorithms to LENA® data (Micheletti et al., 2022; Räsänen et al., 2020), or manually transcribe audio recorded by LENA® devices to improve accuracy or identify behaviors beyond the built-in categories (Bergelson, Casillas, et al., 2019; Mendoza \& Fausey, 2021).

Long-form audio recordings have had a transformational impact on language development research by allowing researchers to characterize opportunities for learning in daily life. Measuring the amount of speech heard by infants in the home (Weisleder \& Fernald, 2013) or in a daycare setting (Perry et al., 2018) revealed individual differences input that predict later vocabulary. Full-day language recording synchronized with other data sources allows researchers to identify how auditory input and vocal production interact with other processes. Beyond individual differences in aggregated data, long-form recordings can be used to determine the temporal schedule of experiences. For example, infants' daily experiences hearing music are clustered in time, with ``bursty'' episodes of hearing music separated by relatively long periods during which music is absent (Mendoza \& Fausey, 2022). Synchronizing audio recordings with other data sources extends researchers' ability to characterize daily experiences. Linking LENA® speech measurements to repeated, time-stamped text-message surveys about infant device placement revealed that infants heard less caregiver speech during moments that they were restrained in devices such as swings, exersaucers, and car seats (Malachowski et al., 2023). A custom-built wearable ECG and audio recorder allowed Wass et al. (2022) to discover that infant arousal increases the likelihood of infant vocalization across the day.

We identified five key features of long-form audio methods that should be replicated in analogous studies of motor behavior. First, wearable audio recorders are \emph{mobile}. Measurement is not limited to a particular room because the recording device travels with the participant. Data are recorded to onboard device memory, so participants do not need to be in range of a receiver. Second, wearable audio recording is \emph{unobtrusive}. Participants' reactivity to observation, such as from a video camera, may influence behavior. For example, caregivers spoke more frequently to infants during a video-recorded portion of a home recording compared with audio-only segments captured by a LENA® device (Bergelson, Amatuni, et al., 2019). Third and fourth, recordings capture \emph{real-time data} over a \emph{full day}. The ability to record real-time data is vital for making inferences about processes that happen on the timescale of minutes or even seconds within in an individual as opposed to comparisons of aggregated data between infants. Synchronizing real-time data to other data streams helps to reveal sources of variability within an individual (e.g., Wass et al., 2022). Full-day recordings are essential for capturing experiences across the heterogeniety of daily routines that moderate behavior (e.g., play, feeding, errands) (Kadooka et al., 2021, April; Tamis-LeMonda et al., 2018). ``Burstiness'' of behavior means that long recordings are needed to capture clusters of events amid long periods in which they may be absent (Barbaro \& Fausey, 2022; Warlaumont et al., 2021). Fifth, \emph{automatic classification} means that the approach can scale to analyze large numbers of participants over long recordings without the bottleneck of manual annotation/transcription. Automatic classification can only replace human annotation if it is sufficiently accurate and unbiased. An independent assessment of the LENA® algorithms found mixed results about the accuracy of different outcomes. For example, correlations between human transcribed counts of adult words and child vocalizations against LENA®'s automatic counts were strong, \emph{r} = .698 and \emph{r} = .649, respectively (Cristia et al., 2020). However, poor agreement was found for other metrics, such as the number of ``conversational turns'' between the child and communicative partners and the identification of male speakers.

Thus, for some use cases (and for some metrics), long form audio recordings provide a mobile, unobtrusive way to automatically score real-time data over a full day. In the remainder of the paper, we turn to the question of how to replicate these qualities in long form recordings of motor behavior.

\hypertarget{limitations-of-video-and-survey-methods}{%
\subsection{Limitations of Video and Survey Methods}\label{limitations-of-video-and-survey-methods}}

Video and survey methods are the current state-of-the-art in assessing motor behavior. Although, each method has advantages and disadvantages for characterizing infants' everyday motor experiences that complements the other, neither method on its own can provide comparable data to the long form audio recordings reviewed in the previous section.

Video observation is the most common way of measuring infant motor behavior in home recordings. Most often, an experimenter with a handheld camera follows infants from room to room to ensure that their movements are visible throughout the recording session (Chen et al., 2022; Herzberg et al., 2021; Karasik et al., 2011). The primary advantage of video recording is that it captures real-time behavior. Infants' body position, locomotion, and reaching comprise events that occur on the timescale of seconds, so standard video recording is adequate to score gross motor behavior. However, requiring an experimenter to operate a camera is obtrusive, whereas relying on a stationary camera means that infants will be absent from view as they move from place to place. Moreover, video observation cannot easily scale to long durations or large numbers of participants. Logistically, an experimenter cannot follow behind infants to record their behavior from morning to night (and were they to do so, they would likely alter infants' and caregivers' behavior). Typical video recording sessions last 45-120 minutes (Chen et al., 2022; Herzberg et al., 2021; Karasik et al., 2011), short of capturing the variety of activities across the full daily routine. Even if full day videos were available, the lack of suitable automatic classification tools means that the human cost of annotation makes it difficult for video methods to scale to testing large numbers of participants. Our annotation of body position takes approximately 2-5 hours to complete for every hour of video (depending on how often infants switch positions), meaning that a full ``waking day'' of approximately 11 hours for a 12-month-old (Galland et al., 2012) could take 22-55 hours of labor to full annotate.

In contrast, survey methods such as daily diaries/inventories or ecological momentary assessment (EMA) are mobile, unobtrusive, can be applied across an entire day, and do not need laborious annotation. Diary studies provide caregivers with logs or structured interviews to estimate from memory how much time infants spend in particular activities (Karasik et al., 2022; Majnemer \& Barr, 2005). Ecological momentary assessment uses text-message or app-based notifications to prompt caregivers to make repeated estimates of infants' behavior over the course of a day (Franchak, 2019; Kadooka et al., 2021, April). And although the responses are valuable in aggregate, survey methods lack the real-time temporal resolution to describe moment-to-moment changes in behavior. For diaries and interviews, limits on caregivers' memory mean that they will report what was most frequent, but cannot remember events that happen on the scale of seconds and minutes. At best, EMA surveys prompt caregivers to make hourly observations; increasing the number of surveys per day would be too burdensome for the respondent. Thus, despite being a useful tool for estimating broad developmental changes and individual differences in infants' motor experiences, survey methods are not suited for capturing within-participant temporal dynamics.

\hypertarget{promise-of-inertial-sensing-methods}{%
\subsection{Promise of Inertial Sensing Methods}\label{promise-of-inertial-sensing-methods}}

Measuring infant movement with inertial movement units (IMUs) is a promising avenue for long form recordings of motor behavior in the home (Barbaro, 2019; Bruijns et al., 2020; Cliff et al., 2009; Lobo et al., 2019). Lightweight sensors (10-30 g) can be embedded in garments to make recordings fully \emph{mobile}, and they are \emph{unobtrusive} because they do not require a researcher's presence nor do they record sensitive audio/video that might influence participants' behavior. Many commercially-available and inexpensive IMUs have \textgreater{} 12 hour battery life with onboard storage to record \emph{real-time}, \emph{full-day} motion data at a sampling rate that is higher than typical video (e.g., 50-100 Hz). Moreover, past work has successfully recorded the rate of leg kicks (e.g., Deng et al., 2019) and activity intensity (Schneller et al., 2017) in infants and children across multiple days.

The open question is whether \emph{automatic classification} is sufficiently accurate to measure movement categories that are relevant to developmental and clinical research, and whether measurement validity is acceptable over long recording periods. IMUs typically contain accelerometers that measure linear acceleration paired with gyroscopes that measure angular acceleration. Unlike motion tracking systems that might be used in a lab, IMUs do not provide data about the position of the body in space. Thus, data processing algorithms are needed to classify the raw sensor data (i.e., linear and angular acceleration timeseries) into meaningful categories (e.g., supine, prone, sitting, upright). The difficulty of the classification task depends on the categories of interest. More basic aspects of movement, such as overall activity intensity, can be identified by taking the magnitude of acceleration (irrespective of direction) and applying thresholds or cut-points to define when a movement has occurred at a particular intensity (Armstrong et al., 2019; e.g., Hager et al., 2017).

Categorizing body position---supine on the back, prone on the belly, sitting, upright, or held off the ground by a caregiver---is too complex for cut-point definitions to be accurate. First, the magnitude of movement can vary greatly \emph{within} a body position. An upright infant can be standing still or can be walking briskly across the room. A prone infant and be stationary in ``tummy time'', or they can crawl in myriad ways (Adolph et al., 1998). Moreover, the configuration of the arms, legs, and torso within a body position can vary greatly in everyday contexts. Infants can sit on the floor in a tripod position with support from an arm, in a ``V'' position with legs fully extended, in a ``W'' position with knees bent. Sitting on a caregivers' lap without the need to maintain balance means that the legs can dangle and the torso can lean in different directions. Sitting in a high chair or car seat reduces the magnitude of postural sway within sitting, and creates even more possibilities for how the arms and legs may move relative to the torso. Indeed, creating an all-encompassing set of rules for how to annotate sitting from video is no trivial task because of the various ways that sitting can occur in daily life. Finally, caregivers frequently pick up and transport infants, creating motion signals that need to be differentiated from independent activity (Kwon et al., 2019; Patel et al., 2019). Thus, modern approaches to human activity recognition have used machine learning to classify activity categories based on features derived from IMU data in adults (Arif \& Kattan, 2015; Preece et al., 2009), children (Nam \& Park, 2013; Ren et al., 2016; Stewart et al., 2018), and infants (Airaksinen et al., 2020; Franchak et al., 2021; Yao et al., 2019). Synchronized video with ground-truth human annotations creates training data for a machine learning algorithm, such as a random forest model, that can be later used to predict categories from IMU data that was not annotated. Crucially, this allows automatic classification to scale to full day recording by relying on a relatively smaller set of video annotation.

Three prior investigations have used machine learning to categorize infant body position from IMUs towards the goal of collecting full-day data. Airaksinen et al. (2020) tested 4- to 8-month-olds in a laboratory visit, and found 95\% accuracy in distinguishing between body position categories that crawling infants could perform on the floor (excluding times that infants were held by caregivers). Using a wider age range of 6-18 months, Franchak et al. (2021) found 98\% accuracy (\emph{kappa} = 95\%) in a laboratory validation study in categorizing body position that included infants who could both crawl and walk and also included a category for caregiver holding. Most recently, Airaksinen et al. (2022) conducted a validation study of body position classification in either a home or clinic testing in 4- to 19-month-olds, refining their previous method to detect moments that infants were carried by caregivers. Classification accuracy did not vary between lab and home settings, and was generally high (95\%, \emph{kappa} = .93). Although all three studies yielded promising classification accuracy, accuracy was assessed in brief (15-60 minute) sessions supervised by a researcher, leaving the open question of how well body position classification will scale to testing across an entire day of natural home life.

\hypertarget{goals-of-the-current-study}{%
\subsection{Goals of the Current Study}\label{goals-of-the-current-study}}

Accordingly, the overarching goal of the current study is to test the feasibility and validity of long-form body position recording in the home during unsupervised, everyday behavior. Supervised recordings from past work (Airaksinen et al., 2022, 2020; Franchak et al., 2021), whether in the home or in the lab, let researchers set up the situation to encourage or restrict certain behaviors. Usually, caregivers are asked to play with the infant. However, across a real day, non-play activities (e.g., eating lunch in a high chair) create challenging situations for applying automated classification of body position. Will models trained on video-recorded observations at the beginning of the day generalize to predict behavior at a later time? Assessing the validity of temporally \emph{distal} periods is a crucial step to establish whether automatic classification can be used to measure body position across a day.

In the current study, we report the feasibility and validity of body position classification over the full day in the home based on 35 testing sessions from 22 infants aged 4-14 months. Participants received a custom pair of infant leggings embedded with 4 IMUs (one on each ankle and one on each hip) and a video camera to collect ground truth data about infant body position. A \emph{proximal comparison} period began when participants received the equipment and completed a guided phone call during which caregivers were asked to elicit different body positions based on prompts from the experimenter. Although not directly supervised, this period was most similar to previous recordings because it occurred during a convenient time for the infant and caregiver to play while they received instructions from the experimenter. The \textbf{first goal} of the current study was to determine the accuracy of body position classification during the proximal comparison period using this novel, semi-supervised procedure in participants' homes.

A second, \emph{distal comparison} period followed the proximal comparison period and captured approximately 90 minutes of home behavior that was completely unsupervised. Caregivers and infants could (and did) do whatever they wished. Because this recording happened a considerable amount of time after the initial setup and instructions from the experimenter, accuracy could decline if caregivers or infants moved the garment or sensors. Moreover, increasing variation in everyday activities during the distal comparison creates a greater challenge for classification, testing whether machine learning models can generalize to novel test cases. Thus, the \textbf{second goal} of our study was to assess accuracy during the distal comparison.

After this second video recording period, we asked caregivers to keep infants wearing IMUs for the rest of the day until their regular bedtime, creating the \textbf{first real-time, full-day dataset of infant body position}. Interpreting such data required caregivers to log when infants napped, when they removed the sensor garment for diaper changes or other reasons, and when infants went to bed at the end of the day. The \textbf{third goal} of the study was to examine the quality of the full-day data. Could body position be successfully recorded over the desired period? Finally, the novel full-day dataset affords us a unique chance to ask whether the estimated time infants spent in different body positions align with past results using video and survey methods. Thus, the \textbf{fourth goal} of the current study was to determine whether full-day body position measurements conformed to expectations about age differences in body position. Based on past results (Franchak, 2019), infants should spend increasingly more time sitting and upright but less time supine over the age range tested (4 to 14 months).

\hypertarget{methods}{%
\section{Methods}\label{methods}}

\hypertarget{participants-and-design}{%
\subsection{Participants and Design}\label{participants-and-design}}

Infants were recruited in one of two age groups: \emph{Younger} infants were between 4 and 7 months and \emph{Older} infants were between 11 and 14 months. There were 9 infants in the 4-7 month group (X female) and 14 in the 11-14 month group (X female). Families were recruited through social media advertisements and from community events in Southern California. The ethnicity for infants was reported as X Hispanix/Latinx and X for not Hispanix/Latinx. Race was reported as X Caucasian, X Asian, X \textasciitilde\textasciitilde\textasciitilde{} and X others. Parents were compensated \$30 in cash for each visit they completed. The BLINDED Institutional Review Board reviewed and approved all procedures associated with the study. All participants gave their informed consent to participate.

Most participants were tested in a single session (\emph{n} = 17), but 6 participants contributed between 2-4 sessions as part of an ongoing longitudinal study. Only 1 session was excluded due to a technical error---one of the four IMU sensors failed to record, resulting in an unusable set of data for classification. Across the two age groups, we report data on a total of 34 sessions, with 14 sessions from younger infants and 20 sessions from older infants. Across sessions, younger infants' age ranged from 3.84 to 7.16 (\emph{M} = 4.99) and older infants' age ranged from 10.74 to 14.23 (\emph{M} = 11.75).

\hypertarget{apparatus}{%
\subsection{Apparatus}\label{apparatus}}

Four inertial movement units (IMUs) were used to record infant movement across the day (MC10 Biostamp). A custom garment was made to hold the IMUs: Internal pockets were sewn into a snug-fitting pair of infant leggings so that IMUs would stay close to the body and so that infants could not pull out the sensors. On each side of the body there was a pocket over the hip and a pocket just above the ankle. Each sensor recorded accelerometer and gyroscope data at 62.5 Hz throughout the day, with sufficient battery and on-device storage to record for approximately 12 hours. Infants also wore a LENA® recorder throughout the day in the front pocket of a LENA® shirt, located near the infant's chest.

Video recordings were captured using an action camera on a miniature tripod (Insta360 ONE R) that caregivers placed in the room that the infant was in. The proprietary ``Boosted Battery Base'' attached to the action camera to allow for a total of 3 hours of recording. However, as described below, this created two video files separated by a variable gap of approximately 40 s. Caregivers also received a log sheet to record times that infants napped as well as times that the sensor garment was removed from the infant (e.g., baths, diaper changes).

\hypertarget{procedure}{%
\subsection{Procedure}\label{procedure}}

\begin{figure}

{\centering \includegraphics[width=0.99\linewidth]{figures/timeline} 

}

\caption{Example Timeline Caption.}\label{fig:exemplar-timeline}
\end{figure}

Figure \ref{fig:exemplar-timeline}) shows an exemplar timeline of the entire procedure and recording periods for a single participant. On the day of the visit, a researcher arrived at the participant's home in the morning between and prepared all the equipment at the doorstep. In order to synchronize all three recording devices, the researcher began by first turning on the video camera and the LENA audio device (the IMUs were already configured and placed at arrival because they required a proprietary sensor dock to begin recording in the laboratory). To create an easily recognizable synchronization point between the video recording and IMU data, the researcher dropped or struck the leggings (containing the IMUs) on a surface in view of the camera, as in (Franchak et al., 2021). All the equipment---once recording and with synchronization information recorded---was placed inside a large bucket and left outside the family's front door.

The researcher then called the caregiver on the phone and walked them through a set of procedures needed to properly set up the equipment and record video for classifier training and testing. At the start of this ``guided call'', the caregiver was instructed to place the camera in an area that captured the majority of the room. Next, they were asked to put the pair of leggings and shirt on their infant, with the researcher providing guidance about how to correctly orient the garments.

Afterwards, the researcher asked the caregiver to complete a number of guided activities with their infant. Within view of the camera, the caregiver was asked to place their infant in several different positions: lying supine, lying prone, sitting on the floor, standing upright, held by the caregiver while the caregiver walked back and forth, crawling, walking, and sitting in a restrained seat (e.g., high chair). Depending on the infants' age and motor skill level, the positions could be done independently or were completed with assistance from the caregiver. The researcher kept time to ensure at least 1 minute of behavior for each activity. Once completed, the caregiver was then instructed to play with their infant for 10 minutes within view of the camera to collect additional training data with the infant in positions that would be typical of play.

Afterwards, they were to go about their day as usual with the infant wearing the sensor garment until their bedtime, only taking off the sensor for naps, baths, and diaper changes. The caregiver logged the times the sensors were removed (blank areas in the timeline in Figure \ref{fig:exemplar-timeline})) or the child took a nap (gray areas in the timeline in Figure \ref{fig:exemplar-timeline})) so that those times could be excluded from analysis. The following day a researcher picked up the equipment, verified the paperwork was signed, and compensated the participant.

Because the camera only had the battery life to record for \textasciitilde3 hours (divided into two 90-minute video files), this divided the day into different periods for analysis. As seen in the bottom of Figure \ref{fig:exemplar-timeline}), the \emph{video period} comprised the first three hours of recording starting from the researcher's arrival when they turned on the camera. The first 90-minute video file, termed the \emph{proximal comparison}, contained the activities during the guided call followed by a period of infants and caregivers resuming their normal activities. Because this video contained the synchronization point, the data in this period had temporal synchrony between IMU and video data that contained errors of no more than 30-60 ms (1-2 video frames). When the first video file ended, a second video file was recorded, termed the \emph{distal comparison}. This video recorded the next 90 minutes of natural activity. However, because there was a variable gap of \textasciitilde40 s between the two videos, temporal synchrony in the distal comparison video was approximate containing offsets of \textasciitilde{} 5-10 s.

\hypertarget{body-position-annotation}{%
\subsection{Body Position Annotation}\label{body-position-annotation}}

The proximal and distal comparison videos for each participant were annotated by trained human coders to identify infant \emph{body position} into one of 5 mutually-exclusive categories: supine, prone, sitting, upright, or held by caregiver. All coding was done using Datavyu software (datavyu.org).

Supine was coded when the infant was lying on their back or was reclined up to a 45 degrees angle. Supine was also coded in the rare cases when the infant was laying on their side. Prone was coded when the infant was lying on their stomach, was on all fours in a downward dog position, or was crawling. We scored sitting to include any form of the following seated positions: 1) infants sat with their buttocks on a surface, such as on the floor or a caregiver's lap, 2) infant was in a kneeling-sit position, in which their knees were on the ground with their legs tucked underneath the buttocks, and 3) infant was in a seating device, such as a high chair, that kept the torso oriented perpendicular to the ground (a reclined position, such as in a young infant's car seat, would be counted as supine). Upright was coded when the infant was standing on the ground with two feet or walking (regardless of whether infants' balance was assisted by a caregiver or with their hands holding onto something for support). When an infant was carried by a caregiver, held was coded. However, when the caregiver was sitting with the infant in their lap the infant's body position was coded as if the caregiver was a surface (e.g., if the infant was sitting on the caregiver's lap this was coded as sitting). Times during the video when the infant was out of view were excluded. Periods when the leggings were being adjusted or taken off the infant were also excluded, as were transitions between body positions.

A primary coder completed annotation for the full length of the video, while an independent reliability coder completed annotation for the first thirty minutes of each video. Interrater reliability was based on the proportion of video frames that the two coders chose the same body position code. Overall agreement averaged XX.X\% across video files, ranging from XX-XX. Cohen's \emph{kappa} ranged from XX-XX across files, with a mean of XX.

\hypertarget{body-position-classification}{%
\subsection{Body position classification}\label{body-position-classification}}

\hypertarget{data-sharing-and-transparency}{%
\subsection{Data Sharing and Transparency}\label{data-sharing-and-transparency}}

\hypertarget{results}{%
\section{Results}\label{results}}

\hypertarget{goal-1-assess-the-proximal-accuracy-of-body-position-classification-models}{%
\subsection{Goal 1: Assess the proximal accuracy of body position classification models}\label{goal-1-assess-the-proximal-accuracy-of-body-position-classification-models}}

\begin{figure}

{\centering \includegraphics{manuscript_files/figure-latex/metrics-1} 

}

\caption{Metrics}\label{fig:metrics}
\end{figure}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:metricstable}Summary statistics for model performance metrics shown separately for group and individual models.}

\begin{tabular}{lllllll}
\toprule
 & \multicolumn{3}{c}{Group} & \multicolumn{3}{c}{Individual} \\
\cmidrule(r){2-4} \cmidrule(r){5-7}
Metric & Median & Mean & SD & Median & Mean & SD\\
\midrule
Overall Accuracy & 0.894 & 0.846 & 0.131 & 0.933 & 0.916 & 0.072\\
Balanced Accuracy & 0.913 & 0.891 & 0.077 & 0.911 & 0.907 & 0.071\\
F1 & 0.822 & 0.816 & 0.106 & 0.882 & 0.865 & 0.091\\
Sensitivity & 0.856 & 0.825 & 0.127 & 0.847 & 0.841 & 0.119\\
Pos Pred Value & 0.826 & 0.810 & 0.125 & 0.928 & 0.899 & 0.107\\
Kappa & 0.768 & 0.746 & 0.161 & 0.849 & 0.821 & 0.143\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\renewcommand{\arraystretch}{.75}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:metricsbyclass}Model performance metrics for each body position category, shown separately for group and individual models.}

\begin{tabular}{llllllll}
\toprule
 &  & \multicolumn{3}{c}{Group} & \multicolumn{3}{c}{Individual} \\
\cmidrule(r){3-5} \cmidrule(r){6-8}
Metric & Position & Median & Mean & SD & Median & Mean & SD\\
\midrule
Balanced Accuracy & Supine & 0.964 & 0.925 & 0.101 & 0.994 & 0.959 & 0.086\\
 & Prone & 0.996 & 0.941 & 0.121 & 0.968 & 0.915 & 0.135\\
 & Sitting & 0.909 & 0.862 & 0.141 & 0.963 & 0.942 & 0.070\\
 & Upright & 0.907 & 0.849 & 0.155 & 0.938 & 0.874 & 0.123\\
 & Held & 0.901 & 0.871 & 0.110 & 0.868 & 0.844 & 0.154\\ \midrule
F1 & Supine & 0.943 & 0.867 & 0.176 & 0.986 & 0.938 & 0.127\\
 & Prone & 0.979 & 0.897 & 0.190 & 0.957 & 0.880 & 0.191\\
 & Sitting & 0.879 & 0.803 & 0.217 & 0.951 & 0.924 & 0.099\\
 & Upright & 0.738 & 0.722 & 0.246 & 0.844 & 0.766 & 0.231\\
 & Held & 0.771 & 0.779 & 0.151 & 0.839 & 0.768 & 0.238\\ \midrule
Sensitivity & Supine & 1.000 & 0.905 & 0.180 & 1.000 & 0.954 & 0.129\\
 & Prone & 1.000 & 0.894 & 0.234 & 0.974 & 0.849 & 0.272\\
 & Sitting & 0.910 & 0.811 & 0.257 & 0.964 & 0.915 & 0.136\\
 & Upright & 0.837 & 0.730 & 0.297 & 0.891 & 0.786 & 0.253\\
 & Held & 0.852 & 0.772 & 0.228 & 0.773 & 0.702 & 0.312\\ \midrule
Pos Pred Value & Supine & 0.995 & 0.828 & 0.292 & 1.000 & 0.932 & 0.134\\
 & Prone & 0.987 & 0.877 & 0.236 & 1.000 & 0.892 & 0.210\\
 & Sitting & 0.896 & 0.802 & 0.261 & 0.972 & 0.945 & 0.079\\
 & Upright & 0.839 & 0.739 & 0.283 & 0.923 & 0.825 & 0.228\\
 & Held & 0.852 & 0.794 & 0.240 & 0.943 & 0.899 & 0.134\\ \midrule
Kappa & Supine & 0.907 & 0.764 & 0.295 & 0.983 & 0.912 & 0.166\\
 & Prone & 0.968 & 0.860 & 0.259 & 0.942 & 0.841 & 0.246\\
 & Sitting & 0.816 & 0.702 & 0.297 & 0.915 & 0.887 & 0.127\\
 & Upright & 0.707 & 0.673 & 0.281 & 0.822 & 0.741 & 0.236\\
 & Held & 0.732 & 0.726 & 0.209 & 0.826 & 0.727 & 0.277\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\hypertarget{goal-2-assess-the-distal-accuracy-of-body-position-classification-models}{%
\subsection{Goal 2: Assess the distal accuracy of body position classification models}\label{goal-2-assess-the-distal-accuracy-of-body-position-classification-models}}

\begin{figure}

{\centering \includegraphics{manuscript_files/figure-latex/part2overall-1} 

}

\caption{Overall agreement between human-coded body position and model-predicted body position in the distal comparison. Agreement for group models is shown in (A) and agreement for individual models is shown in (B). Plots are shown separately for each body position with a reference line that indicates perfect agreement; each point in a plot represent data for a single participant. The three outlier participants are plotted in dark gray, with a different shape marking each individual.}\label{fig:part2overall}
\end{figure}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:pt2overalltable}Correlations between human-coded and model-predicted body position durations across the entire long delay period. Correlations are provided within each posture and overall, and computed separately using group and individual models with and without outlier participants.}

\begin{tabular}{lllll}
\toprule
 & \multicolumn{2}{c}{With Outliers} & \multicolumn{2}{c}{Without Outliers} \\
\cmidrule(r){2-3} \cmidrule(r){4-5}
Position & Group & Individual & Group & Individual\\
\midrule
Held & 0.02 & 0.04 & 0.73 & 0.60\\
Prone & 0.97 & 0.86 & 0.97 & 0.84\\
Sitting & 0.79 & 0.97 & 0.91 & 0.95\\
Supine & 0.88 & 0.98 & 0.94 & 0.97\\
Upright & 0.63 & 0.83 & 0.99 & 0.95\\ \midrule
Overall & 0.80 & 0.91 & 0.95 & 0.96\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\begin{figure}

{\centering \includegraphics{manuscript_files/figure-latex/part2bins-1} 

}

\caption{Prediction performance (difference in minutes between human-coded and model-predicted body position) for 10-minute bins in the distal comparison. Each point shows the mean and SE for a single participant for each body position, summarizing the prediction difference for each of their 10-minute bins. Points falling within the gray shaded region indicate that average prediction errors were less than 1 minute. Performance is plotted separately for (A) group models and (B) individual models. The three outlier participants are plotted in dark gray, with a different shape marking each individual.}\label{fig:part2bins}
\end{figure}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:pt2binstable}Correlations between human-coded and model-predicted body position durations using 10-minute bins during the distal comparison. Correlations are provided within each posture and overall, and computed separately using group and individual models with and without outlier participants.}

\begin{tabular}{lllll}
\toprule
 & \multicolumn{2}{c}{With Outliers} & \multicolumn{2}{c}{Without Outliers} \\
\cmidrule(r){2-3} \cmidrule(r){4-5}
Position & Group & Individual & Group & Individual\\
\midrule
Held & 0.51 & 0.46 & 0.67 & 0.63\\
Prone & 0.96 & 0.90 & 0.96 & 0.89\\
Sitting & 0.72 & 0.93 & 0.89 & 0.92\\
Supine & 0.76 & 0.96 & 0.88 & 0.93\\
Upright & 0.91 & 0.93 & 0.98 & 0.96\\ \midrule
Overall & 0.80 & 0.94 & 0.92 & 0.94\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

What can we learn about the two outliers? What do they tell us about how this will work in the future.

One outlier (106-4) had significant confusion between sitting and supine. Both their proximal and distal performance was bad, but only for sitting and supine. Watching the videos showed that this participant spent a long period of time in a seating device that was reclined between supine and sitting. Also spent long time in mother's arms in a similar supine/sitting position that was hard to classify.

The other outlier (107-3) had good proximal accuracy but poor distal accuracy. Confusion between upright and suspended. Infant was in a baby walker for most of part 2. Moved more like a baby held by a CG than a baby walking upright.

For both, other class performance was good, suggesting that it's about the particular events, not about the quality of the algorithm in applying to the participant overall.

\hypertarget{goal-3-examine-the-data-quality-of-full-day-home-recordings}{%
\subsection{Goal 3: Examine the data quality of full-day home recordings}\label{goal-3-examine-the-data-quality-of-full-day-home-recordings}}

\begin{figure}

{\centering \includegraphics{manuscript_files/figure-latex/timelines-1} 

}

\caption{Timelines}\label{fig:timelines}
\end{figure}

\hypertarget{goal-4-assess-the-suitability-of-full-day-predictions-for-capturing-age-differences-in-body-position}{%
\subsection{Goal 4: Assess the suitability of full-day predictions for capturing age differences in body position}\label{goal-4-assess-the-suitability-of-full-day-predictions-for-capturing-age-differences-in-body-position}}

\begin{figure}

{\centering \includegraphics{manuscript_files/figure-latex/age-1} 

}

\caption{Age trends}\label{fig:age}
\end{figure}

\begin{table}[tbp]

\begin{center}
\begin{threeparttable}

\caption{\label{tab:agetable}Summary of age differences in full-day body position for younger (4- to 7-month) and older (11- to 14-month) infants. Values shown are the mean percent of time for each body position averaged across infants in each group. Standard deviations are shown in parentheses. Descriptive statistics are shown separately for group and individual models.}

\begin{tabular}{lllll}
\toprule
 & \multicolumn{2}{c}{Group} & \multicolumn{2}{c}{Individual} \\
\cmidrule(r){2-3} \cmidrule(r){4-5}
Position & Younger & Older & Younger & Older\\
\midrule
Upright & 7.7\% (9.3) & 18.6\% (7.4) & 10.4\% (13.6) & 18.7\% (8.4)\\
Sitting & 24.9\% (11.5) & 44.4\% (10.1) & 18.8\% (16.3) & 46.9\% (13.3)\\
Prone & 14.4\% (13.8) & 14.4\% (6.0) & 12.5\% (10.2) & 16.9\% (10.6)\\
Supine & 38.5\% (24.0) & 14.0\% (8.4) & 40.2\% (31.6) & 10.0\% (9.5)\\
Held & 12.7\% (7.1) & 8.5\% (5.4) & 17.1\% (20.8) & 7.4\% (7.6)\\
\bottomrule
\end{tabular}

\end{threeparttable}
\end{center}

\end{table}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\newpage

\hypertarget{references}{%
\section{References}\label{references}}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-AdolphCTL2014}{}}%
Adolph, K. E., \& Tamis-LeMonda, C. S. (2014). The costs and benefits of development: The transition from crawling to walking. \emph{Child Development Perspectives}, \emph{8}, 187--192. \url{https://doi.org/10.1111/cdep.12085}

\leavevmode\vadjust pre{\hypertarget{ref-AdolphVereijken1998}{}}%
Adolph, K. E., Vereijken, B., \& Denny, M. A. (1998). Learning to crawl. \emph{Child Development}, \emph{69}, 1299--1312. \url{https://doi.org/10.1111/j.1467-8624.1998.tb06213.x}

\leavevmode\vadjust pre{\hypertarget{ref-AiraksinenGallen2022}{}}%
Airaksinen, M., Gallen, A., Kivi, A., Vijayakrishnan, P., Häyrinen, T., Ilén, E., Räsänen, O., Haataja, L. M., \& Vanhatalo, S. (2022). Intelligent wearable allows out-of-the-lab tracking of developing motor abilities in infants. \emph{Communications Medicine}, \emph{2}(1). \url{https://doi.org/10.1038/s43856-022-00131-6}

\leavevmode\vadjust pre{\hypertarget{ref-AiraksinenRasanen2020}{}}%
Airaksinen, M., Räsänen, O., Ilén, E., Häyrinen, T., Kivi, A., Marchi, V., Gallen, A., Blom, S., Varhe, A., Kaartinen, N., et al. (2020). Automatic posture and movement tracking of infants with wearable movement sensors. \emph{Scientific Reports}, \emph{10}(1), 1--13.

\leavevmode\vadjust pre{\hypertarget{ref-Arif2015}{}}%
Arif, M., \& Kattan, A. (2015). Physical activities monitoring using wearable acceleration sensors attached to the body. \emph{{PLoS ONE}}, \emph{10}, e0130851.

\leavevmode\vadjust pre{\hypertarget{ref-ArmstrongCovington2019}{}}%
Armstrong, B., Covington, L. B., Hager, E. R., \& Black, M. M. (2019). Objective sleep and physical activity using 24-hour ankle-worn accelerometry among toddlers from low-income families. \emph{Sleep Health}, \emph{5}(5), 459--465.

\leavevmode\vadjust pre{\hypertarget{ref-Barbaro2019}{}}%
Barbaro, K. de. (2019). Automated sensing of daily activity: A new lens into development. \emph{Developmental Psychobiology}, \emph{61}(3), 444--464.

\leavevmode\vadjust pre{\hypertarget{ref-BarbaroFausey2022}{}}%
Barbaro, K. de, \& Fausey, C. M. (2022). Ten lessons about infants' everyday experiences. \emph{Current Directions in Psychological Science}, \emph{31}(1), 28--33. \url{https://doi.org/10.1177/09637214211059536}

\leavevmode\vadjust pre{\hypertarget{ref-Bergelson2019Input}{}}%
Bergelson, E., Amatuni, A., Dailey, S., Koorathota, S., \& Tor, S. (2019). Day by day, hour by hour: Naturalistic language input to infants. \emph{Developmental Science}, \emph{22}, e12715.

\leavevmode\vadjust pre{\hypertarget{ref-Bergelson2019Corpus}{}}%
Bergelson, E., Casillas, M., Soderstrom, M., Seidl, A., Warlaumont, A. S., \& Amatuni, A. (2019). What do {N}orth {A}merican babies hear? {A} large-scale cross-corpus analysis. \emph{Developmental Science}, \emph{22}, e12724.

\leavevmode\vadjust pre{\hypertarget{ref-BruijnsTruelove2020}{}}%
Bruijns, B. A., Truelove, S., Johnson, A. M., Gilliland, J., \& Tucker, P. (2020). Infants' and toddlers' physical activity and sedentary time as measured by accelerometry: A systematic review and meta-analysis. \emph{International Journal of Behavioral Nutrition and Physical Activity}, \emph{17}(1), 14.

\leavevmode\vadjust pre{\hypertarget{ref-ChenSchneider2022}{}}%
Chen, Q., Schneider, J. L., West, K. L., \& Iverson, J. M. (2022). Infant locomotion shapes proximity to adults during everyday play in the u.s. \emph{Infancy}. \url{https://doi.org/10.1111/infa.12503}

\leavevmode\vadjust pre{\hypertarget{ref-CliffReilly2009}{}}%
Cliff, D. P., Reilly, J. J., \& Okely, A. D. (2009). Methodological considerations in using accelerometers to assess habitual physical activity in children aged 0--5 years. \emph{Journal of Science and Medicine in Sport}, \emph{12}(5), 557--567.

\leavevmode\vadjust pre{\hypertarget{ref-CristiaLavechin2020}{}}%
Cristia, A., Lavechin, M., Scaff, C., Soderstrom, M., Rowland, C., Räsänen, O., Bunce, J., \& Bergelson, E. (2020). A thorough evaluation of the language environment analysis ({LENA}) system. \emph{Behavior Research Methods}, \emph{53}(2), 467--486. \url{https://doi.org/10.3758/s13428-020-01393-5}

\leavevmode\vadjust pre{\hypertarget{ref-DengTrujillo-Priego2019}{}}%
Deng, W., Trujillo-Priego, I. A., \& Smith, B. A. (2019). How many days are necessary to represent an infant{\textquotesingle}s typical daily leg movement behavior using wearable sensors? \emph{Physical Therapy}, \emph{99}(6), 730--738. \url{https://doi.org/10.1093/ptj/pzz036}

\leavevmode\vadjust pre{\hypertarget{ref-Survey}{}}%
Franchak, J. M. (2019). Changing opportunities for learning in everyday life: Infant body position over the first year. \emph{Infancy}, \emph{24}, 187--209.

\leavevmode\vadjust pre{\hypertarget{ref-CurrentOpinion}{}}%
Franchak, J. M. (2020). The ecology of infants' perceptual-motor exploration. \emph{Current Opinion in Psychology}, \emph{32}, 110--114.

\leavevmode\vadjust pre{\hypertarget{ref-Freeplay}{}}%
Franchak, J. M., Kretch, K. S., \& Adolph, K. E. (2018). See and be seen: Infant-caregiver social looking during locomotor free play. \emph{Developmental Science}, \emph{21}, e12626.

\leavevmode\vadjust pre{\hypertarget{ref-FranchakScott2021}{}}%
Franchak, J. M., Scott, V., \& Luo, C. (2021). A contactless method for measuring full-day, naturalistic motor behavior using wearable inertial sensors. \emph{Frontiers in Psychology}, \emph{12}. \url{https://doi.org/10.3389/fpsyg.2021.701343}

\leavevmode\vadjust pre{\hypertarget{ref-GallandTaylor2012}{}}%
Galland, B. C., Taylor, B. J., Elder, D. E., \& Herbison, P. (2012). Normal sleep patterns in infants and children: A systematic review of observational studies. \emph{Sleep Medicine Reviews}, \emph{16}(3), 213--222. \url{https://doi.org/10.1016/j.smrv.2011.06.001}

\leavevmode\vadjust pre{\hypertarget{ref-HagerTilton2017}{}}%
Hager, E., Tilton, N., Wang, Y., Kapur, N., Arbaiza, R., Merry, B., \& Black, M. (2017). The home environment and toddler physical activity: An ecological momentary assessment study. \emph{Pediatric Obesity}, \emph{12}(1), 1--9.

\leavevmode\vadjust pre{\hypertarget{ref-HerzbergFletcher2021}{}}%
Herzberg, O., Fletcher, K. K., Schatz, J. L., Adolph, K. E., \& Tamis-LeMonda, C. S. (2021). Infant exuberant object play at home: Immense amounts of time-distributed, variable practice. \emph{Child Development}, \emph{93}(1), 150--164.

\leavevmode\vadjust pre{\hypertarget{ref-KadookaCaufield2021}{}}%
Kadooka, K., Caufield, M., Fausey, C. M., \& Franchak, J. M. (2021, April). Visuomotor learning opportunities are nested within everyday activities. \emph{Paper Presented at the Biennial Meeting of the Society for Research in Child Development}.

\leavevmode\vadjust pre{\hypertarget{ref-KarasikKuchirko2022}{}}%
Karasik, L. B., Kuchirko, Y., Dodojonova, R. M., \& Elison, J. T. (2022). Comparison of {U.S.} And {Tajik} infants' time in containment devices. \emph{Infant and Child Development}, \emph{31}(4). \url{https://doi.org/10.1002/icd.2340}

\leavevmode\vadjust pre{\hypertarget{ref-Karasik2011}{}}%
Karasik, L. B., Tamis-LeMonda, C. S., \& Adolph, K. E. (2011). Transition from crawling to walking and infants' actions with objects and people. \emph{Child Development}, \emph{82}, 1199--1209. \url{https://doi.org/10.1111/j.1467-8624.2011.01595.x}

\leavevmode\vadjust pre{\hypertarget{ref-Karasik2014}{}}%
Karasik, L. B., Tamis-LeMonda, C. S., \& Adolph, K. E. (2014). Crawling and walking infants elicit different verbal responses from mothers. \emph{Developmental Science}, \emph{17}, 388--395. \url{https://doi.org/10.1111/desc.12129}

\leavevmode\vadjust pre{\hypertarget{ref-CWW}{}}%
Kretch, K. S., Franchak, J. M., \& Adolph, K. E. (2014). Crawling and walking infants see the world differently. \emph{Child Development}, \emph{85}, 1503--1518. \url{https://doi.org/10.1111/cdev.12206}

\leavevmode\vadjust pre{\hypertarget{ref-KwonZavos2019}{}}%
Kwon, S., Zavos, P., Nickele, K., Sugianto, A., \& Albert, M. V. (2019). Hip and wrist-worn accelerometer data analysis for toddler activities. \emph{International Journal of Environmental Research and Public Health}, \emph{16}(14), 2598.

\leavevmode\vadjust pre{\hypertarget{ref-LoboHall2019}{}}%
Lobo, M. A., Hall, M. L., Greenspan, B., Rohloff, P., Prosser, L. A., \& Smith, B. A. (2019). Wearables for pediatric rehabilitation: How to optimally design and use products to meet the needs of users. \emph{Physical Therapy}, \emph{99}(6), 647--657.

\leavevmode\vadjust pre{\hypertarget{ref-LuoFranchak2020}{}}%
Luo, C., \& Franchak, J. M. (2020). Head and body structure infants' visual experiences during mobile, naturalistic play. \emph{{PLoS ONE}}, \emph{15}, e0242009.

\leavevmode\vadjust pre{\hypertarget{ref-MajnemerBarr2005}{}}%
Majnemer, A., \& Barr, R. G. (2005). Influence of supine sleep positioning on early motor milestone acquisition. \emph{Developmental Medicine and Child Neurology}, \emph{47}, 370--376.

\leavevmode\vadjust pre{\hypertarget{ref-MalachowskiSalo2023}{}}%
Malachowski, L. G., Salo, V. C., Needham, A. W., \& Humphreys, K. L. (2023). Infant placement and language exposure in daily life. \emph{Infant and Child Development}. \url{https://doi.org/10.1002/icd.2405}

\leavevmode\vadjust pre{\hypertarget{ref-MendozaFausey2021}{}}%
Mendoza, J. K., \& Fausey, C. M. (2021). Everyday music in infancy. \emph{Developmental Science}, \emph{24}(6). \url{https://doi.org/10.1111/desc.13122}

\leavevmode\vadjust pre{\hypertarget{ref-MendozaFausey2022}{}}%
Mendoza, J. K., \& Fausey, C. M. (2022). Everyday parameters for episode-to-episode dynamics in the daily music of infancy. \emph{Cognitive Science}, \emph{46}(8). \url{https://doi.org/10.1111/cogs.13178}

\leavevmode\vadjust pre{\hypertarget{ref-MichelettiYao2022}{}}%
Micheletti, M., Yao, X., Johnson, M., \& Barbaro, K. de. (2022). Validating a model to detect infant crying from naturalistic audio. \emph{Behavior Research Methods}. \url{https://doi.org/10.3758/s13428-022-01961-x}

\leavevmode\vadjust pre{\hypertarget{ref-NamPark2013}{}}%
Nam, Y., \& Park, J. W. (2013). Child activity recognition based on cooperative fusion model of a triaxial accelerometer and a barometric pressure sensor. \emph{IEEE Journal of Biomedical and Health Informatics}, \emph{17}, 420--426.

\leavevmode\vadjust pre{\hypertarget{ref-PatelShi2019}{}}%
Patel, P., Shi, Y., Hajiaghajani, F., Biswas, S., \& Lee, M.-H. (2019). A novel two-body sensor system to study spontaneous movements in infants during caregiver physical contact. \emph{Infant Behavior and Development}, \emph{57}, 101383. \url{https://doi.org/10.1016/j.infbeh.2019.101383}

\leavevmode\vadjust pre{\hypertarget{ref-PerryPrince2018}{}}%
Perry, L. K., Prince, E. B., Valtierra, A. M., Rivero-Fernandez, C., Ullery, M. A., Katz, L. F., Laursen, B., \& Messinger, D. S. (2018). A year in words: The dynamics and consequences of language experiences in an intervention classroom. \emph{{PLOS} {ONE}}, \emph{13}(7), e0199893. \url{https://doi.org/10.1371/journal.pone.0199893}

\leavevmode\vadjust pre{\hypertarget{ref-Preece2009}{}}%
Preece, S. J., Goulermas, J. Y., Kenney, L. P. J., \& Howard, D. (2009). A comparison of feature extraction methods for the classification of dynamic activities from accelerometer data. \emph{IEEE Transactions on Biomedical Engineering}, \emph{56}, 871--879.

\leavevmode\vadjust pre{\hypertarget{ref-RasanenSeshadri2020}{}}%
Räsänen, O., Seshadri, S., Lavechin, M., Cristia, A., \& Casillas, M. (2020). {ALICE}: An open-source tool for automatic measurement of phoneme, syllable, and word counts from child-centered daylong recordings. \emph{Behavior Research Methods}, \emph{53}(2), 818--835. \url{https://doi.org/10.3758/s13428-020-01460-x}

\leavevmode\vadjust pre{\hypertarget{ref-RenDing2016}{}}%
Ren, X., Ding, W., Crouter, S. E., Mu, Y., \& Xie, R. (2016). Activity recognition and intensity estimation in youth from accelerometer data aided by machine learning. \emph{Applied Intelligence}, \emph{45}(2), 512--529.

\leavevmode\vadjust pre{\hypertarget{ref-SchnellerBentsen2017}{}}%
Schneller, M. B., Bentsen, P., Nielsen, G., Brønd, J. C., Ried-Larsen, M., Mygind, E., \& Schipperijn, J. (2017). Measuring children's physical activity. \emph{Medicine \& Science in Sports \& Exercise}, \emph{49}(6), 1261--1269. \url{https://doi.org/10.1249/mss.0000000000001222}

\leavevmode\vadjust pre{\hypertarget{ref-SoskaAdolph2014}{}}%
Soska, K. C., \& Adolph, K. E. (2014). Postural position constrains multimodal object exploration in infants. \emph{Infancy}, \emph{19}, 138--161. \url{https://doi.org/10.1111/infa.12039}

\leavevmode\vadjust pre{\hypertarget{ref-StewartNarayanan2018}{}}%
Stewart, T., Narayanan, A., Hedayatrad, L., Neville, J., Mackay, L., \& Duncan, S. (2018). A dual-accelerometer system for classifying physical activity in children and adults. \emph{Medicine and Science in Sports and Exercise}, \emph{50}(12), 2595--2602.

\leavevmode\vadjust pre{\hypertarget{ref-Tamis-LeMondaCustode2018}{}}%
Tamis-LeMonda, C. S., Custode, S., Kuchirko, Y., Escobar, K., \& Lo, T. (2018). Routine language: Speech directed to infants during home activities. \emph{Child Development}, \emph{90}(6), 2135--2152. \url{https://doi.org/10.1111/cdev.13089}

\leavevmode\vadjust pre{\hypertarget{ref-Thurman2017}{}}%
Thurman, S. L., \& Corbetta, D. (2017). Spatial exploration and changes in infant-mother dyads around transitions in infant locomotion. \emph{Developmental Psychology}, \emph{53}, 1207--1221.

\leavevmode\vadjust pre{\hypertarget{ref-WarlaumontSobowale2021}{}}%
Warlaumont, A. S., Sobowale, K., \& Fausey, C. M. (2021). Daylong mobile audio recordings reveal multitimescale dynamics in infants' vocal productions and auditory experiences. \emph{Current Directions in Psychological Science}, \emph{31}(1), 12--19. \url{https://doi.org/10.1177/09637214211058166}

\leavevmode\vadjust pre{\hypertarget{ref-WassPhillips2022}{}}%
Wass, S., Phillips, E., Smith, C., Fatimehin, E. O., \& Goupil, L. (2022). Vocal communication is tied to interpersonal arousal coupling in caregiver-infant dyads. \emph{{eLife}}, \emph{11}. \url{https://doi.org/10.7554/elife.77399}

\leavevmode\vadjust pre{\hypertarget{ref-Weisleder2013}{}}%
Weisleder, A., \& Fernald, A. (2013). Talking to children matters: Early language experience strengthens processing and builds vocabulary. \emph{Psychological Science}, \emph{24}, 2143--2152.

\leavevmode\vadjust pre{\hypertarget{ref-WestIverson2021}{}}%
West, K. L., \& Iverson, J. M. (2021). Communication changes when infants begin to walk. \emph{Developmental Science}, \emph{24}(5). \url{https://doi.org/10.1111/desc.13102}

\leavevmode\vadjust pre{\hypertarget{ref-YaoPlotz2019}{}}%
Yao, X., Plötz, T., Johnson, M., \& Barbaro, K. de. (2019). Automated detection of infant holding using wearable sensing: Implications for developmental science and intervention. \emph{Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies}, \emph{3}(2), 1--17.

\end{CSLReferences}


\end{document}
